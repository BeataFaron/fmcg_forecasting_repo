{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3a881893-b29a-4dbd-8ce5-b01aac76b212",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# FMCG Forecasting: Weekly Prediction Job\n",
    "\n",
    "This notebook simulates a **weekly prediction job** for FMCG demand forecasting.  \n",
    "It performs real-time inference using a previously trained model and logs the predictions back to the Feature Store.\n",
    "\n",
    "## Objectives\n",
    "\n",
    "1. **Load a new daily batch** of sales data for SKU `MI-006`\n",
    "2. Retrieve the **last 7 days of history** from the Feature Store\n",
    "3. Apply the same feature transformations using `engineer_features_daily`\n",
    "4. Generate predictions using the registered model\n",
    "5. Save batch-level predictions to:\n",
    "   - Parquet (for archiving)\n",
    "   - CSV (for dashboards)\n",
    "   - Feature Store (for traceability and future retraining)\n",
    "\n",
    "> This notebook represents a **production-like scoring job**, typically triggered weekly by an orchestration tool (e.g., Airflow, dbutils.jobs.runNow, or Databricks Workflows).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b7ed8448-b21a-46b6-8695-ad803f44a4a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2ccf5d5-80b5-45c4-ab5b-80c39554fecb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from utils.forecasting_utils import engineer_features_daily\n",
    "from utils.weekly_helpers import load_model_from_registry, generate_batch_id, format_output_paths\n",
    "from databricks.feature_store import FeatureStoreClient\n",
    "from databricks.feature_store.entities.feature_lookup import FeatureLookup\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "from pyspark.sql.functions import col, min as min_\n",
    "from datetime import timedelta\n",
    "from pyspark.sql.types import DateType, IntegerType\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3ead9738-e4bd-42ca-af32-1160dc108524",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Load new batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c82a2b16-22fb-473d-a451-61ae95f2aa67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_batch = spark.read.parquet(\"dbfs:/FileStore/fmcg/batch_MI_006_2025_01_06.parquet\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9149d26a-d802-4bbd-a336-ea10976aac00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## set Cut-Off Date & load 7 days back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83aae558-9fff-4fde-b85f-1d1309d554f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cutoff_batch_date = df_batch.select(min_(\"date\")).first()[0]\n",
    "cutoff_start_date = cutoff_batch_date - timedelta(days=7)\n",
    "\n",
    "fs = FeatureStoreClient()\n",
    "\n",
    "df_history = fs.read_table(\"fmcg_features_daily\") \\\n",
    "    .filter((col(\"date\") >= cutoff_start_date) & (col(\"date\") < cutoff_batch_date))\n",
    "\n",
    "display(df_history)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "26709c50-7bf7-42ff-aff7-769230a46209",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Set the same schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "817b9339-ac3f-48dd-be4b-d7d88b7ab518",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_batch_prepared = df_batch \\\n",
    "    .withColumn(\"date\", col(\"date\").cast(DateType())) \\\n",
    "    .withColumn(\"promotion_flag\", col(\"promotion_flag\").cast(IntegerType())) \\\n",
    "    .withColumn(\"delivery_days\", col(\"delivery_days\").cast(IntegerType())) \\\n",
    "    .withColumn(\"stock_available\", col(\"stock_available\").cast(IntegerType())) \\\n",
    "    .withColumn(\"delivered_qty\", col(\"delivered_qty\").cast(IntegerType())) \\\n",
    "    .withColumn(\"units_sold\", col(\"units_sold\").cast(IntegerType()))\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "for col_name in [\"lag_1\", \"lag_2\", \"rolling_mean_4\", \"rolling_std_4\", \"momentum\", \"avg_by_channel_region\"]:\n",
    "    df_batch_prepared = df_batch_prepared.withColumn(col_name, lit(None).cast(\"double\"))\n",
    "\n",
    "\n",
    "df_batch_prepared = df_batch_prepared.select(df_history.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5203afaf-41b5-4030-a423-f8d6119720bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Union & create features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e703ee0c-c4b3-4b13-80eb-929d45ff5330",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_combined = df_history.unionByName(df_batch_prepared)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "975aba90-4882-468b-90be-ebe9c1da7af8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_combined_fe = engineer_features_daily(df_combined)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7841967-a446-4c64-92bf-61cc52442bb1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_combined_fe.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d7d3f39-763f-494b-807f-7e09f693dc93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Prepare & Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06abf31d-4601-4ae2-9f49-15af5d8268b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "features = [\n",
    "    \"lag_1\", \"lag_2\", \"rolling_mean_4\", \"rolling_std_4\", \n",
    "    \"momentum\", \"avg_by_channel_region\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f468afd4-9eee-4fcf-a7ca-9a6c86441bec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql.functions import struct\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "from databricks.feature_store import FeatureStoreClient\n",
    "import mlflow\n",
    "\n",
    "# 1. Filtruj dane batch\n",
    "df_new_only = df_combined_fe.filter(col(\"date\") >= cutoff_batch_date)\n",
    "\n",
    "# 2. Konwertuj do Pandas do predykcji\n",
    "df_pandas = df_new_only.select(features).toPandas()\n",
    "\n",
    "# 3. Załaduj model\n",
    "model_uri = 'runs:/139e23d4770d4356bc025f3035b9576b/model'\n",
    "loaded_model = mlflow.pyfunc.load_model(model_uri)\n",
    "\n",
    "# 4. Predykcja\n",
    "df_pandas[\"units_sold_pred\"] = loaded_model.predict(df_pandas)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "706d9d5e-3659-4451-804c-8e7a6a6ae4a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Save Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efbb44a6-4956-4d5f-8baf-6c62d4cfbef7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Użyj daty batcha do stworzenia nazwy pliku\n",
    "formatted_date = cutoff_batch_date.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Finalne predykcje połączone z danymi wejściowymi\n",
    "df_pred_full = df_new_only.toPandas()\n",
    "df_pred_full[\"units_sold_pred\"] = df_pandas[\"units_sold_pred\"]\n",
    "\n",
    "# Ścieżki do zapisania\n",
    "output_parquet_path = f\"dbfs:/FileStore/fmcg/predictions/final_daily_preds_{formatted_date}.parquet\"\n",
    "output_csv_path = f\"/dbfs/FileStore/fmcg/predictions/final_daily_preds_{formatted_date}.csv\"\n",
    "\n",
    "# Zapis do Parquet (overwrite → nadpisuje predykcje dla tej daty, jeśli już istnieją)\n",
    "spark.createDataFrame(df_pred_full).write.mode(\"overwrite\").parquet(output_parquet_path)\n",
    "print(f\"✅ Saved predictions to Parquet: {output_parquet_path}\")\n",
    "\n",
    "# Zapis do CSV (opcjonalny – przydatny np. do dashboardu)\n",
    "df_pred_full.to_csv(output_csv_path, index=False)\n",
    "print(f\"✅ Also saved as CSV: {output_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e26f91c4-8bd8-40e7-b35d-c83db8f0896f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Log new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8b22640-7fea-4273-bbf2-c207ec13f21c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.feature_store import FeatureStoreClient\n",
    "\n",
    "fs = FeatureStoreClient()\n",
    "\n",
    "# Zapisz do Feature Store z użyciem merge i kluczy (np. sku + date)\n",
    "fs.write_table(\n",
    "    name=\"fmcg_features_daily\",\n",
    "    df=spark.createDataFrame(df_pred_full),\n",
    "    mode=\"merge\"\n",
    ")\n",
    "print(\"✅ Merged new predictions into Feature Store (fmcg_features_daily)\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "05_weekly_prediction_job",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
